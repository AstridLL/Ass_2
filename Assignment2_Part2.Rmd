---
title: "Computational Modeling - Week 5 - Assignment 2 - Part 2"
author: "Riccardo Fusaroli"
date: "2/19/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
getwd()
locpath= getwd()
setwd(locpath)

library(pacman)
p_load(plyr, dplyr, stringr, tidyverse, tidyr, ggplot2, pastecs, lmerTest, MuMIn, lme4, modelr, Metrics, caret, ddalpha, ggplot2, pastecs, crqa, lmerTest, pROC, modelr) # choose n (y/n?) in ddalpha
library(rethinking)
```

## In this assignment we learn how to assess rates from a binomial distribution, using the case of assessing your teachers' knowledge of CogSci.

### Second part: Focusing on predictions

Last year you assessed the teachers (darned time runs quick!). Now you want to re-test them and assess whether your models are producing reliable predictions. In Methods 3 we learned how to do machine-learning style assessment of predictions (e.g. rmse on testing datasets). Bayesian stats makes things a bit more complicated. So we'll try out how that works. N.B. You can choose which prior to use for the analysis of last year's data.

Questions to be answered (but see guidance below):
1- Write a paragraph discussing how assessment of prediction performance is different in Bayesian vs. frequentist models
####
When asssesing a model we look at three different measures (maybe other word):
The structure of the model (values and robustness), the model's predictions, and the robustness of the model. 
In frequentist statistics, when looking at the structure of the model, we look at the beta values, the std. error (SE) and the p value. Meanwhile in Bayesian statistics we look at the posterior of the parameters. When asessing the predictions of the model in frequentist statistics we look at the residuals and the RMSE (average error/distance predicted by the model to the actual data. The standard deviation of the residuals).Meanwhile in Bayesian statistics we look at the predictive posterior (which we can use to search for biases both in the fitted data as well as in new data) (somthing about samples). 
"det er noget med at i frequentist får man en 'prediction', altså vores model forudsiger noget bestemt, hvorimod Bayesian forudsiger en probability distribution af forskellige values. Altså, hvor sandsynligt det er, at hver værdi sker."
When asessing the robustness of the model 
#####

FROM SLIDES; NR 5 
#####
A model should be assessed:
Looking at its structure (values and robustness)
Beta values, SE, p?
Here Posterior (of the parameters)

Looking at its predictions
Residuals/ROC/RMSE;
Predictive Posterior (of the outcome)
Either searching for biases on the fitted data
Or searching for biases on new data

Robustness of the model
e.g. cross-validated parameters
here different priors
Comparative cross-validation based information criteria

######

 
2- Provide at least one plot and one written line discussing prediction errors for each of the teachers.



This is the old data:
- Riccardo: 3 correct answers out of 6 questions
- Kristian: 2 correct answers out of 2 questions (then he gets bored)
- Josh: 160 correct answers out of 198 questions (Josh never gets bored)
- Mikkel: 66 correct answers out of 132 questions

This is the new data:
- Riccardo: 9 correct answers out of 10 questions (then he freaks out about teaching preparation and leaves)
- Kristian: 8 correct answers out of 12 questions
- Josh: 148 correct answers out of 172 questions (again, Josh never gets bored)
- Mikkel: 34 correct answers out of 65 questions

Guidance Tips

1. There are at least two ways of assessing predictions 
2. Last year's results are this year's expectations.
3. Are the parameter estimates changing? (way 1 - using the posterior as a prior)
4. How does the new data look in last year's predictive posterior? (way 2 - "calculating" the predictive posterior by means of samples and comparing it's predictions to the new data)

We can look at the 

```{r}
#####      WAY 1    ########
# OLD DATA 

# grid approximation
dens = 20 
p_grid = seq(from = 0, to = 1, length.out = dens)
# define the prior 
prior = rep(1, dens) # flat (equal possibility for all outcomes)
# prior2 = dnorm(p_grid, mean = 0.8, sd = 0.2)
# Compute the likelihood at each value in grid
R_likelihood <- dbinom( 3 , size=6 , prob=p_grid ) # first number how many 1's(corrects), second number how many possible 1's. 
# Compute the posterior (likelihood by prior)
R_unstd.posterior <- R_likelihood * prior
# Standardize the posterior (so it sums to 1)
R_posterior <- R_unstd.posterior / sum(R_unstd.posterior)
# Draw the plot
Data=data.frame(grid=p_grid,posterior=R_posterior,prior=prior,likelihood=R_likelihood)
ggplot(Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior/dens),color='red')+  xlab("probability of correct answer/knowledge of cogsci")+ ylab("posterior probability")

# NEW DATA INCLUDED (USING THE OLD DATA AS A PRIOR)

prior2 = R_posterior# dnorm(p_grid, mean = 0.8, sd = 0.2)
# Compute the likelihood at each value in grid
R_likelihood1 <- dbinom( 9 , size=10 , prob=p_grid ) # first number how many 1's(corrects), second number how many possible 1's. 
# Compute the posterior (likelihood by prior)
R_unstd.posterior1 <- R_likelihood1 * prior2
# Standardize the posterior (so it sums to 1)
R_posterior1 <- R_unstd.posterior1 / sum(R_unstd.posterior1)
# Draw the plot
Data=data.frame(grid=p_grid,posterior=R_posterior1,prior=prior2,likelihood=R_likelihood1)
ggplot(Data,aes(grid,posterior))+  geom_point()+geom_line()+theme_classic()+  geom_line(aes(grid,prior),color='red')+  xlab("probability of correct answer/knowledge of cogsci")+ ylab("posterior probability")

#####      WAY 2    ########
#Identifying the most probable range of parameter values
samples = sample(p_grid, prob = R_posterior, size = 1e4, replace = TRUE)
plot(samples) 
dens(samples)
HPDI( samples , prob=0.5 ) 

# We need to sample all possible p’s and then weight them (multiply them) with the posterior likelihood of each value of p. 
c <- rbinom( 1e4 , size=10 , prob=samples) 
dens(c)
table(c)/1e4
simplehist(c, xlab = "predicted correct answers based on simulations")
# in this histogram we can see the predicted correct answers of Riccardo, with the most proberble scenario being that he answers 5/10 correct (though only with a probability of 15% as the distribution is so wide because he only answered 6 questions and therefore not enough to be more sure that he would answer half correct again). 



```

# feedback in class
It can be a good model but not describe the phenomenon that is happening during the data collection /second test. could be that the first round answers were only due to a heavy previous coffee drinking for a teacher and if not the same scenario the next time - no caffeine, then it wont capture the same phenomenon that were in play the last time, and the model looks bad. 
somehting - asses whether there is a statistical change /estimate if there is, if we find that one value between the confidence intercals and the mean of the model or somehting (middle og estimates)
to camporae data- out it in a plot with the data of the number.
difference: one new datapaint
e.g. set on the parameters  until 
1.diablrf



### Depending on time: Questions from the handbook
2H1. Suppose there are two species of panda bear. Both are equally common in the wild and live in the same places. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. They differ however in their family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of field research.
Now suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins?

2H2. Recall all the facts from the problem above. Now compute the probability that the panda we have is from species A, assuming we have observed only the first birth and that it was twins.

2H3. Continuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A.

2H4. A common boast of Bayesian statisticians is that Bayesian inference makes it easy to use all of the data, even if the data are of different types. So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information you have about the test:
- The probability it correctly identifies a species A panda is 0.8.
- The probability it correctly identifies a species B panda is 0.65.
The vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. Then redo your calculation, now using the birth data as well.
